"""
ImageProcessingLectures_app.py
Interactive Image Processing Lectures using Streamlit + OpenCV + NumPy + Pillow

Run:
    pip install streamlit opencv-python-headless numpy pillow
    streamlit run ImageProcessingLectures_app.py

Structure: single-file app with 9 lecture modules implemented as functions.
"""

import streamlit as st
import cv2
import numpy as np
from PIL import Image
import io
import os
from typing import Tuple

st.set_page_config(page_title="Image Processing Lectures", layout="wide")

# --------------------- Utilities ---------------------
@st.cache_resource
def load_sample_image(path: str = None) -> np.ndarray:
    """Load a sample image from disk. If path is None, generate a synthetic image."""
    if path and os.path.exists(path):
        img = Image.open(path).convert("RGB")
        return np.array(img)
    # synthetic sample: gradient + shapes
    h, w = 512, 768
    base = np.zeros((h, w, 3), dtype=np.uint8)
    # gradient
    for i in range(h):
        color = int(255 * (i / h))
        base[i, :, :] = (color // 2, color, 255 - color)
    # add a rectangle and circle
    cv2.rectangle(base, (50, 50), (250, 200), (0, 255, 0), -1)
    cv2.circle(base, (500, 300), 80, (255, 0, 0), -1)
    return base


def to_pil(img: np.ndarray) -> Image.Image:
    return Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) if img.ndim == 3 else Image.fromarray(img)


def read_image(uploaded_file) -> np.ndarray:
    image = Image.open(uploaded_file).convert('RGB')
    return np.array(image)


def show_before_after(img_before: np.ndarray, img_after: np.ndarray, caption_before: str = "Before", caption_after: str = "After"):
    col1, col2 = st.columns(2)
    with col1:
        st.image(img_before, caption=caption_before, use_column_width=True)
    with col2:
        st.image(img_after, caption=caption_after, use_column_width=True)


def save_image_to_bytes(img: np.ndarray, fmt: str = 'PNG') -> bytes:
    pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) if img.ndim == 3 else Image.fromarray(img)
    buf = io.BytesIO()
    pil.save(buf, format=fmt)
    return buf.getvalue()


def ensure_bgr(img: np.ndarray) -> np.ndarray:
    # convert RGB (PIL) -> BGR (OpenCV style) if needed
    if img.ndim == 3 and img.shape[2] == 3:
        # PIL->numpy gives RGB, convert to BGR for OpenCV ops convenience
        return cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    return img

# --------------------- App Layout ---------------------
st.title("ğŸ“š Ø³Ù„Ø³Ù„Ø© Ù…Ø­Ø§Ø¶Ø±Ø§Øª ØªÙØ§Ø¹Ù„ÙŠØ© ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±")
st.sidebar.header("Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª")
lectures = [
    "Ù…Ø­Ø§Ø¶Ø±Ø© 1: Ù…Ø¯Ø®Ù„ ÙˆÙ…Ø¹Ù…Ø§Ø±ÙŠØ© Ø§Ù„ØµÙˆØ± Ø§Ù„Ø±Ù‚Ù…ÙŠØ©",
    "Ù…Ø­Ø§Ø¶Ø±Ø© 2: Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø£Ù„ÙˆØ§Ù†",
    "Ù…Ø­Ø§Ø¶Ø±Ø© 3: Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙƒØ³Ù„",
    "Ù…Ø­Ø§Ø¶Ø±Ø© 4: Ø§Ù„ÙÙ„Ø§ØªØ± ÙˆØ§Ù„Ø§Ù„ØªÙØ§Ù",
    "Ù…Ø­Ø§Ø¶Ø±Ø© 5: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡",
    "Ù…Ø­Ø§Ø¶Ø±Ø© 6: ÙƒØ´Ù Ø§Ù„Ø­ÙˆØ§Ù",
    "Ù…Ø­Ø§Ø¶Ø±Ø© 7: Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©",
    "Ù…Ø­Ø§Ø¶Ø±Ø© 8: Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ©",
    "Ù…Ø­Ø§Ø¶Ø±Ø© 9: Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø®ØªØ§Ù…ÙŠ"
]
choice = st.sidebar.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø©", lectures)

# Upload image / or use sample
st.sidebar.markdown("---")
use_sample = st.sidebar.checkbox("Ø§Ø³ØªØ®Ø¯Ù… ØµÙˆØ±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© (Ø¹Ù†Ø¯ Ø¹Ø¯Ù… Ø±ÙØ¹ ØµÙˆØ±Ø©)", value=True)
uploaded = st.sidebar.file_uploader("Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© (jpg, png)", type=["jpg", "jpeg", "png"])
if uploaded:
    user_img = read_image(uploaded)
else:
    user_img = load_sample_image()

# Convert to BGR for OpenCV processing; keep a copy of original RGB for display consistency
orig_rgb = user_img.copy()
orig_bgr = cv2.cvtColor(orig_rgb, cv2.COLOR_RGB2BGR) if orig_rgb.ndim == 3 else orig_rgb.copy()

# Utility: display image info
def image_info(img: np.ndarray):
    if img is None:
        return
    st.write(f"**Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ (Height Ã— Width Ã— Channels):** {img.shape}")
    st.write(f"**Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** {img.dtype}")
    # estimate bit depth from dtype
    bit_depth = {
        'uint8': 8,
        'uint16': 16,
        'float32': 32
    }.get(str(img.dtype), 'unknown')
    st.write(f"**Ø§Ù„Ø¹Ù…Ù‚ Ø§Ù„Ù„ÙˆÙ†ÙŠ (ØªÙ‚Ø±ÙŠØ¨ÙŠ):** {bit_depth} bits")

# --------------------- Lecture Implementations ---------------------

# Lecture 1
def lecture1(img_rgb: np.ndarray):
    st.header("ğŸ“˜ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© 1: Ù…Ø¯Ø®Ù„ ÙˆÙ…Ø¹Ù…Ø§Ø±ÙŠØ© Ø§Ù„ØµÙˆØ± Ø§Ù„Ø±Ù‚Ù…ÙŠØ©")
    st.markdown(
        """
- Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…ØµÙÙˆÙØ© Ù…Ù† Ø§Ù„Ø¨ÙƒØ³Ù„Ø§ØªØŒ ÙˆÙƒÙ„ Ø¨ÙƒØ³Ù„ ÙŠÙÙ…Ø«Ù„ Ù‚ÙŠÙ…Ø© Ø£Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù‚ÙŠÙ… ØªØ­Ø¯Ø¯ Ø§Ù„Ù„ÙˆÙ†.
- Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ (Height Ã— Width Ã— Channels) ØªØ­Ø¯Ø¯ Ø¯Ù‚Ø© Ø§Ù„ØµÙˆØ±Ø© ÙˆØ´ÙƒÙ„Ù‡Ø§.
- Ø§Ù„Ø¹Ù…Ù‚ Ø§Ù„Ù„ÙˆÙ†ÙŠ (bit depth) ÙŠØ­Ø¯Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ù„ÙƒÙ„ Ù‚Ù†Ø§Ø© (Ù…Ø«Ù„Ø§Ù‹ 8-bit â†’ 0â€“255).
- ØµÙŠØº Ø§Ù„Ù…Ù„ÙØ§Øª (JPEG, PNG) ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¶ØºØ· ÙˆØ§Ù„Ø¬ÙˆØ¯Ø©.
- ÙÙ‡Ù… ØªÙ…Ø«ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù‡Ù… Ù‚Ø¨Ù„ ØªØ·Ø¨ÙŠÙ‚ Ø£ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø©.
        """
    )

    st.subheader("Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    st.write("Ø±ÙØ¹ ØµÙˆØ±Ø© / Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙˆØ±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© â†’ Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØµÙˆØ±Ø© â†’ Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©")
    image_info(img_rgb)
    st.image(img_rgb, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©", use_column_width=True)

# Lecture 2
def lecture2(img_rgb: np.ndarray):
    st.header("ğŸ¨ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© 2: Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø£Ù„ÙˆØ§Ù† (Color Spaces)")
    st.markdown(
        """
- Ø£Ø´Ù‡Ø± Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø£Ù„ÙˆØ§Ù†: RGB (Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø´Ø§Ø´Ø§Øª)ØŒ Gray (Ø£Ø­Ø§Ø¯ÙŠ)ØŒ HSV (Ù…ÙÙŠØ¯ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù„ÙˆÙ†/ØªØ´Ø¨Ø¹/Ø³Ø·ÙˆØ¹).
- Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠÙ† Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ù…ÙÙŠØ¯ Ù„ØªØ¨Ø³ÙŠØ· Ù…Ù‡Ø§Ù… Ù…Ø«Ù„ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø£Ù„ÙˆØ§Ù† Ù…Ø­Ø¯Ø¯Ø© Ø£Ùˆ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ©.
- Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª (OpenCV) ØªØ³ØªØ®Ø¯Ù… BGR Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹.
        """
    )

    st.subheader("Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    mode = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„ØªØ­ÙˆÙŠÙ„", ["RGB (Ø£ØµÙ„ÙŠØ©)", "Gray", "HSV", "ÙØµÙ„ Ù‚Ù†ÙˆØ§Øª R/G/B"])
    bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)

    if mode == "RGB (Ø£ØµÙ„ÙŠØ©)":
        out = img_rgb
    elif mode == "Gray":
        gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)
        out = gray
    elif mode == "HSV":
        hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)
        # convert HSV to RGB for display (scale okay)
        out = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
    else:
        b, g, r = cv2.split(bgr)
        # stack as images for display
        r_img = cv2.merge([r, r, r])
        g_img = cv2.merge([g, g, g])
        b_img = cv2.merge([b, b, b])
        st.write("### Ù‚Ù†ÙˆØ§Øª R / G / B")
        cols = st.columns(3)
        cols[0].image(cv2.cvtColor(r_img, cv2.COLOR_BGR2RGB), caption="R", use_column_width=True)
        cols[1].image(cv2.cvtColor(g_img, cv2.COLOR_BGR2RGB), caption="G", use_column_width=True)
        cols[2].image(cv2.cvtColor(b_img, cv2.COLOR_BGR2RGB), caption="B", use_column_width=True)
        return

    show_before_after(img_rgb, out, "Ø§Ù„Ø£ØµÙ„ÙŠØ©", "Ø§Ù„Ù†ØªÙŠØ¬Ø©")

# Lecture 3
def lecture3(img_rgb: np.ndarray):
    st.header("âš™ï¸ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© 3: Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙƒØ³Ù„ (Point Operations)")
    st.markdown(
        """
- ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø³Ø·ÙˆØ¹: Ø¥Ø¶Ø§ÙØ©/Ø·Ø±Ø­ Ù‚ÙŠÙ…Ø© Ù…Ù† ÙƒÙ„ Ø¨ÙƒØ³Ù„.
- ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªØ¨Ø§ÙŠÙ†: Ø¶Ø±Ø¨/ØªØºÙŠÙŠØ± Ù…Ø¯Ù‰ Ø§Ù„Ù‚ÙŠÙ….
- Ø§Ù„ØµÙˆØ± Ø§Ù„Ø³Ø§Ù„Ø¨Ø©: 255 - Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¨ÙƒØ³Ù„.
- Thresholding: ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø«Ù†Ø§Ø¦ÙŠØ© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¹ØªØ¨Ø©.
        """
    )

    st.subheader("Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)

    brightness = st.slider("Ø§Ù„Ø³Ø·ÙˆØ¹ (Ù…Ø¶Ø§Ù)", -100, 100, 0)
    contrast = st.slider("Ø§Ù„ØªØ¨Ø§ÙŠÙ† (Ù…Ø¶Ø§Ø¹Ù)", 50, 200, 100)
    apply_negative = st.button("ØªØ·Ø¨ÙŠÙ‚ Negative")

    # apply brightness & contrast
    alpha = contrast / 100.0
    beta = brightness
    adjusted = cv2.convertScaleAbs(bgr, alpha=alpha, beta=beta)

    if apply_negative:
        neg = 255 - adjusted
        result = neg
    else:
        result = adjusted

    st.write("### Thresholding")
    thresh_type = st.selectbox("Ù†ÙˆØ¹ Threshold", ["Binary", "Otsu"])
    gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)
    if thresh_type == "Binary":
        t = st.slider("Ø§Ù„Ø¹ØªØ¨Ø©", 0, 255, 127)
        _, binary = cv2.threshold(gray, t, 255, cv2.THRESH_BINARY)
    else:
        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    show_before_after(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB), cv2.cvtColor(result, cv2.COLOR_BGR2RGB), "Ø§Ù„Ø£ØµÙ„ÙŠØ©", "Ø¨Ø¹Ø¯ Ø§Ù„Ø³Ø·ÙˆØ¹/Ø§Ù„ØªØ¨Ø§ÙŠÙ†/Ø§Ù„Ø³Ù„Ø¨ÙŠ")
    st.image(binary, caption="Ø§Ù„Ù€ Threshold (Ø«Ù†Ø§Ø¦ÙŠØ©)", use_column_width=True)

# Lecture 4

def lecture4(img_rgb: np.ndarray):
    st.header("ğŸ§ª Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© 4: Ø§Ù„ÙÙ„Ø§ØªØ± ÙˆØ§Ù„Ø§Ù„ØªÙØ§Ù (Filtering & Convolution)")
    st.markdown(
        """
- Ø§Ù„ÙÙ„Ø§ØªØ± ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Kernel/Mask ØªÙØ·Ø¨Ù‚ Ø¹Ø¨Ø± Ø§Ù„Ø§Ù„ØªÙØ§Ù (Convolution).
- Ø£Ù…Ø«Ù„Ø©: Blur (ØªÙ†Ø¹ÙŠÙ…)ØŒ Sharpen (Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­ÙˆØ§Ù)ØŒ Emboss (ØªØ£Ø«ÙŠØ± Ù†Ø­Øª).
- ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ­ÙƒÙ… Ø¨Ø­Ø¬Ù… Kernel ÙÙŠ Gaussian/Median.
        """
    )

    st.subheader("Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
    choice = st.selectbox("Ø§Ø®ØªØ± ÙÙ„ØªØ±Ù‹Ø§", ["Original", "Gaussian Blur", "Median Blur", "Sharpen", "Emboss", "Edge (Laplacian)"])

    if choice == "Original":
        out = bgr
    elif choice == "Gaussian Blur":
        k = st.slider("Kernel size (odd)", 1, 31, 5, step=2)
        out = cv2.GaussianBlur(bgr, (k, k), 0)
    elif choice == "Median Blur":
        k = st.slider("Kernel size (odd)", 1, 31, 5, step=2)
        out = cv2.medianBlur(bgr, k)
    elif choice == "Sharpen":
        kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
        out = cv2.filter2D(bgr, -1, kernel)
    elif choice == "Emboss":
        kernel = np.array([[ -2, -1, 0], [-1, 1, 1], [0, 1, 2]])
        out = cv2.filter2D(bgr, -1, kernel) + 128
    else:
        lap = cv2.Laplacian(cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY), cv2.CV_64F)
        lap = np.uint8(np.absolute(lap))
        out = cv2.cvtColor(lap, cv2.COLOR_GRAY2BGR)

    show_before_after(img_rgb, cv2.cvtColor(out, cv2.COLOR_BGR2RGB), "Ø§Ù„Ø£ØµÙ„ÙŠØ©", choice)

# Lecture 5

def add_salt_pepper(img: np.ndarray, amount=0.004, s_vs_p=0.5):
    out = img.copy()
    h, w = out.shape[:2]
    num_salt = np.ceil(amount * h * w * s_vs_p)
    num_pepper = np.ceil(amount * h * w * (1.0 - s_vs_p))

    # Salt
    coords = [np.random.randint(0, i - 1, int(num_salt)) for i in out.shape[:2]]
    out[coords[0], coords[1]] = 255

    # Pepper
    coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in out.shape[:2]]
    out[coords[0], coords[1]] = 0
    return out


def add_gaussian_noise(img: np.ndarray, mean=0, var=10):
    sigma = var ** 0.5
    gauss = np.random.normal(mean, sigma, img.shape).reshape(img.shape)
    noisy = img + gauss
    noisy = np.clip(noisy, 0, 255).astype(np.uint8)
    return noisy


def lecture5(img_rgb: np.ndarray):
    st.header("ğŸ”Š Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© 5: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ (Denoising)")
    st.markdown(
        """
- Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡: Salt & PepperØŒ Gaussian.
- ÙÙ„ØªØ± Median Ø¬ÙŠØ¯ Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ø§Ù„Ù…Ù„Ø­ ÙˆØ§Ù„ÙÙ„ÙÙ„.
- Bilateral filter ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙˆØ§Ù Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ø¹ÙŠÙ….
        """
    )

    bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
    add_noise = st.checkbox("Ø£Ø¶Ù Ø¶ÙˆØ¶Ø§Ø¡ ØµÙ†Ø§Ø¹ÙŠØ© Ù„Ù„ØµÙˆØ±Ø©ØŸ")
    noisy = bgr.copy()
    if add_noise:
        noise_type = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡", ["Salt & Pepper", "Gaussian"])
        if noise_type == "Salt & Pepper":
            amount = st.slider("Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ (amount)", 0.001, 0.05, 0.004, step=0.001)
            noisy = add_salt_pepper(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB), amount=amount)
            noisy = cv2.cvtColor(noisy, cv2.COLOR_RGB2BGR)
        else:
            var = st.slider("Ø§Ù„ØªØ¨Ø§ÙŠÙ† (var)", 1, 100, 10)
            noisy = add_gaussian_noise(bgr, var=var)

    method = st.selectbox("Ø§Ø®ØªØ± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ†Ù‚ÙŠØ©", ["Median", "Bilateral", "Gaussian Blur"])
    if method == "Median":
        k = st.slider("Kernel (odd)", 1, 31, 5, step=2)
        denoised = cv2.medianBlur(noisy, k)
    elif method == "Bilateral":
        d = st.slider("d (pixel diameter)", 1, 50, 9)
        sigmaColor = st.slider("sigmaColor", 1, 200, 75)
        sigmaSpace = st.slider("sigmaSpace", 1, 200, 75)
        denoised = cv2.bilateralFilter(noisy, d, sigmaColor, sigmaSpace)
    else:
        k = st.slider("Kernel size (odd)", 1, 31, 5, step=2)
        denoised = cv2.GaussianBlur(noisy, (k, k), 0)

    show_before_after(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB), cv2.cvtColor(denoised, cv2.COLOR_BGR2RGB), "Ø§Ù„Ø£ØµÙ„ÙŠØ©", "Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ù‚ÙŠØ©")

# Lecture 6

def lecture6(img_rgb: np.ndarray):
    st.header("ğŸ§­ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© 6: ÙƒØ´Ù Ø§Ù„Ø­ÙˆØ§Ù (Edge Detection)")
    st.markdown(
        """
- Ø§Ù„Ø­ÙˆØ§Ù ØªÙ…Ø«Ù„ ØªØºÙŠÙ‘Ø±Ø§Øª Ø³Ø±ÙŠØ¹Ø© ÙÙŠ Ø´Ø¯Ø© Ø§Ù„ØµÙˆØ±Ø©.
- Ø·Ø±Ù‚ Ø´Ø§Ø¦Ø¹Ø©: Sobel (ØªØ¯Ø±Ø¬Ø§Øª x/y)ØŒ LaplacianØŒ ÙˆCanny (Ø·Ø±Ù‚ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø®Ø·ÙˆØ§Øª).
- Canny ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¹ØªØ¨Ø§Øª Ù…Ù†Ø®ÙØ¶Ø© ÙˆØ¹Ø§Ù„ÙŠØ©.
        """
    )

    bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
    method = st.selectbox("Ø§Ø®ØªØ± Ø·Ø±ÙŠÙ‚Ø© ÙƒØ´Ù Ø§Ù„Ø­ÙˆØ§Ù", ["Sobel", "Laplacian", "Canny"])

    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)
    if method == "Sobel":
        k = st.slider("Kernel size (odd)", 1, 31, 3, step=2)
        sx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=k)
        sy = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=k)
        mag = np.sqrt(sx ** 2 + sy ** 2)
        mag = np.uint8(np.clip(mag / mag.max() * 255, 0, 255))
        out = mag
    elif method == "Laplacian":
        lap = cv2.Laplacian(gray, cv2.CV_64F)
        out = np.uint8(np.absolute(lap))
    else:
        low = st.slider("Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© (low)", 0, 255, 50)
        high = st.slider("Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© (high)", 0, 255, 150)
        out = cv2.Canny(gray, low, high)

    show_before_after(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB), out if out.ndim == 2 else cv2.cvtColor(out, cv2.COLOR_BGR2RGB), "Ø§Ù„Ø£ØµÙ„ÙŠØ©", f"Edges: {method}")

# Lecture 7

def lecture7(img_rgb: np.ndarray):
    st.header("ğŸ”¬ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© 7: Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ© (Morphological Ops)")
    st.markdown(
        """
- Ø¹Ù…Ù„ÙŠØ§Øª Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ© ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ© (Binary) Ù…Ø«Ù„ Erosion ÙˆDilation.
- ØªÙØ³ØªØ®Ø¯Ù… Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ø§Ù„ØµØºÙŠØ±Ø©ØŒ ØªØ¹Ø¨Ø¦Ø© Ø§Ù„ÙØ¬ÙˆØ§ØªØŒ ÙØµÙ„ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§ØªØŒ Ø¥Ù„Ø®.
        """
    )

    bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)
    t = st.slider("Ø¹ØªØ¨Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø«Ù†Ø§Ø¦ÙŠØ©", 0, 255, 127)
    _, binary = cv2.threshold(gray, t, 255, cv2.THRESH_BINARY)

    op = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù…ÙˆØ±ÙÙˆÙ„ÙˆØ¬ÙŠØ©", ["Erosion", "Dilation", "Opening", "Closing"])
    ksize = st.slider("Ø­Ø¬Ù… Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø¨Ù†Ø§Ø¦ÙŠ (odd)", 1, 31, 3, step=2)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (ksize, ksize))

    if op == "Erosion":
        result = cv2.erode(binary, kernel, iterations=1)
    elif op == "Dilation":
        result = cv2.dilate(binary, kernel, iterations=1)
    elif op == "Opening":
        result = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
    else:
        result = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)

    show_before_after(binary, result, "Binary", op)

# Lecture 8

def lecture8(img_rgb: np.ndarray):
    st.header("ğŸ” Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© 8: Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ© (Geometric Transforms)")
    st.markdown(
        """
- ØªØ­ÙˆÙŠÙ„Ø§Øª Ù‡Ù†Ø¯Ø³ÙŠØ© ØªØ´Ù…Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø©ØŒ Ø§Ù„Ø¯ÙˆØ±Ø§Ù†ØŒ Ø§Ù„ØªÙƒØ¨ÙŠØ±/Ø§Ù„ØªØµØºÙŠØ±ØŒ Ø§Ù„Ø§Ù†Ø¹ÙƒØ§Ø³ØŒ ÙˆØ§Ù„Ù‚Øµ.
- Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª ØªØºÙŠØ± Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„Ø¨ÙƒØ³Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØµÙÙˆÙØ§Øª ØªØ­ÙˆÙŠÙ„.
        """
    )

    bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
    op = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„ØªØ­ÙˆÙŠÙ„", ["Rotate", "Scale (Zoom)", "Translate", "Flip", "Crop"])

    h, w = bgr.shape[:2]
    if op == "Rotate":
        angle = st.slider("Ø§Ù„Ø²Ø§ÙˆÙŠØ© (Ø¯Ø±Ø¬Ø©)", -180, 180, 0)
        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)
        out = cv2.warpAffine(bgr, M, (w, h))
    elif op == "Scale (Zoom)":
        fx = st.slider("ØªÙƒØ¨ÙŠØ± X", 0.1, 3.0, 1.0)
        fy = st.slider("ØªÙƒØ¨ÙŠØ± Y", 0.1, 3.0, 1.0)
        out = cv2.resize(bgr, None, fx=fx, fy=fy, interpolation=cv2.INTER_LINEAR)
    elif op == "Translate":
        tx = st.slider("ØªØ­Ø±ÙŠÙƒ X (px)", -w, w, 0)
        ty = st.slider("ØªØ­Ø±ÙŠÙƒ Y (px)", -h, h, 0)
        M = np.float32([[1, 0, tx], [0, 1, ty]])
        out = cv2.warpAffine(bgr, M, (w, h))
    elif op == "Flip":
        mode = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ø§Ù†Ø¹ÙƒØ§Ø³", ["Ø¹Ù…ÙˆØ¯ÙŠ", "Ø£ÙÙ‚ÙŠ", "ÙƒÙ„ÙŠ"])
        if mode == "Ø¹Ù…ÙˆØ¯ÙŠ":
            out = cv2.flip(bgr, 0)
        elif mode == "Ø£ÙÙ‚ÙŠ":
            out = cv2.flip(bgr, 1)
        else:
            out = cv2.flip(bgr, -1)
    else:  # Crop
        st.write("Ø§Ø³Ø­Ø¨ Ù…Ø³ØªØ·ÙŠÙ„ Ø§Ù„Ù‚Øµ (Ø£Ø¯Ø®Ù„ Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª)")
        x1 = st.number_input("x1", min_value=0, max_value=w - 1, value=0)
        y1 = st.number_input("y1", min_value=0, max_value=h - 1, value=0)
        x2 = st.number_input("x2", min_value=1, max_value=w, value=w)
        y2 = st.number_input("y2", min_value=1, max_value=h, value=h)
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
        out = bgr[y1:y2, x1:x2]

    # show
    show_before_after(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB), cv2.cvtColor(out, cv2.COLOR_BGR2RGB) if out.ndim == 3 else out, "Ø§Ù„Ø£ØµÙ„ÙŠØ©", op)

# Lecture 9 - Pipeline

def apply_pipeline(bgr: np.ndarray, pipeline: list) -> np.ndarray:
    img = bgr.copy()
    for op in pipeline:
        name = op[0]
        params = op[1] if len(op) > 1 else {}
        if name == 'Gray':
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
        elif name == 'Blur':
            k = params.get('k', 5)
            img = cv2.GaussianBlur(img, (k, k), 0)
        elif name == 'Median':
            k = params.get('k', 5)
            img = cv2.medianBlur(img, k)
        elif name == 'Edges':
            low = params.get('low', 50)
            high = params.get('high', 150)
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, low, high)
            img = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        elif name == 'Thresh':
            t = params.get('t', 127)
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            _, binary = cv2.threshold(gray, t, 255, cv2.THRESH_BINARY)
            img = cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)
        elif name == 'Sharpen':
            kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
            img = cv2.filter2D(img, -1, kernel)
        # add more ops as needed
    return img


def lecture9(img_rgb: np.ndarray):
    st.header("ğŸ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© 9: Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø®ØªØ§Ù…ÙŠ â€” Ø¨Ù†Ø§Ø¡ Pipeline")
    st.markdown(
        """
- Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©.
- Ø§Ø®ØªØ± Ø³Ù„Ø³Ù„Ø© Ø¹Ù…Ù„ÙŠØ§Øª (Ù…Ø«Ù„Ø§Ù‹: Gray â†’ Blur â†’ Edges).
- Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙˆØ¥Ù…ÙƒØ§Ù†ÙŠØ© ØªÙ†Ø²ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©.
        """
    )

    bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)

    ops = st.multiselect("Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ (Ø§Ø¶ØºØ· ÙˆØ­Ø±Ù‘Ùƒ Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨)", ['Gray', 'Blur', 'Median', 'Sharpen', 'Thresh', 'Edges'])
    # For simplicity, collect a single parameter for all where needed
    pipeline = []
    for op in ops:
        if op == 'Blur':
            k = st.slider('Blur kernel (for Blur)', 1, 31, 5, step=2)
            pipeline.append((op, {'k': k}))
        elif op == 'Median':
            k = st.slider('Median kernel (for Median)', 1, 31, 5, step=2)
            pipeline.append((op, {'k': k}))
        elif op == 'Thresh':
            t = st.slider('Threshold value (for Thresh)', 0, 255, 127)
            pipeline.append((op, {'t': t}))
        elif op == 'Edges':
            low = st.slider('Canny low threshold', 0, 255, 50)
            high = st.slider('Canny high threshold', 0, 255, 150)
            pipeline.append((op, {'low': low, 'high': high}))
        else:
            pipeline.append((op, {}))

    if st.button('ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù€ Pipeline'):
        out_bgr = apply_pipeline(bgr, pipeline)
        out_rgb = cv2.cvtColor(out_bgr, cv2.COLOR_BGR2RGB)
        show_before_after(img_rgb, out_rgb, 'Ø§Ù„Ø£ØµÙ„ÙŠØ©', 'Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©')

        # download
        buf = save_image_to_bytes(out_bgr, fmt='PNG')
        st.download_button("ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (PNG)", data=buf, file_name='result.png', mime='image/png')

# --------------------- Dispatcher ---------------------

lecture_funcs = {
    lectures[0]: lecture1,
    lectures[1]: lecture2,
    lectures[2]: lecture3,
    lectures[3]: lecture4,
    lectures[4]: lecture5,
    lectures[5]: lecture6,
    lectures[6]: lecture7,
    lectures[7]: lecture8,
    lectures[8]: lecture9,
}

# run selected lecture
lecture_funcs[choice](orig_rgb)

st.sidebar.markdown("---")
st.sidebar.write("Ø¨ÙÙ†ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø©: Ù…Ø­Ù…Ø¯ â€” Ù…Ø´Ø±ÙˆØ¹ ØªØ¹Ù„ÙŠÙ…ÙŠ Ù„Ù…Ø§Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±")

# Footer
st.write("\n---\n*Ù…Ù„Ø§Ø­Ø¸Ø©:* Ù‡Ø°Ù‡ Ù†Ø³Ø®Ø© Ø£ÙˆÙ„ÙŠØ©. ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ÙˆØªØµØºÙŠØ±/ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø¥Ù„Ù‰ Ù…Ù„ÙØ§Øª Ù…Ù†ÙØµÙ„Ø© `modules/` Ù„Ùˆ Ø£Ø±Ø¯Øª ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¥Ù†ØªØ§Ø¬ÙŠ.")
